{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Aug  2 23:10:41 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 552.22                 Driver Version: 552.22         CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                     TCC/WDDM  | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3050 ...  WDDM  |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   66C    P8              6W /   35W |      14MiB /   4096MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A     38532    C+G   ...a\\AppData\\Roaming\\Zoom\\bin\\Zoom.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "11.7\n",
      "['sm_37', 'sm_50', 'sm_60', 'sm_61', 'sm_70', 'sm_75', 'sm_80', 'sm_86', 'compute_37']\n",
      "(8, 6)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.get_arch_list())\n",
    "print(torch.cuda.get_device_capability())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from lstm_model import LSTMModel\n",
    "from lstm_model import BiLSTMModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "def make_dataset_before_tokenizer(filename, window_size):\n",
    "    # filename = r'sentence_file/2022gendai_info.csv'\n",
    "    df = pd.read_csv(filename)\n",
    "\n",
    "    # window_size = 1, 2, 3, 4\n",
    "    \n",
    "    if window_size == 4:\n",
    "        df['b_text'] = df[['b4', 'b3', 'b2', 'b1']].apply(lambda x: ' '.join(x.dropna().astype(str)), axis=1)\n",
    "        df['a_text'] = df[['a1', 'a2', 'a3', 'a4']].apply(lambda x: ' '.join(x.dropna().astype(str)), axis=1)\n",
    "    elif window_size == 3:\n",
    "        df['b_text'] = df[['b3', 'b2', 'b1']].apply(lambda x: ' '.join(x.dropna().astype(str)), axis=1)\n",
    "        df['a_text'] = df[['a1', 'a2', 'a3']].apply(lambda x: ' '.join(x.dropna().astype(str)), axis=1)\n",
    "    elif window_size == 2:\n",
    "        df['b_text'] = df[['b2', 'b1']].apply(lambda x: ' '.join(x.dropna().astype(str)), axis=1)\n",
    "        df['a_text'] = df[['a1', 'a2']].apply(lambda x: ' '.join(x.dropna().astype(str)), axis=1)\n",
    "    else:\n",
    "        df['b_text'] = df[['b1']].apply(lambda x: ' '.join(x.dropna().astype(str)), axis=1)\n",
    "        df['a_text'] = df[['a1']].apply(lambda x: ' '.join(x.dropna().astype(str)), axis=1)\n",
    "    \n",
    "    df['pose'] = '@@@'\n",
    "    \n",
    "    # データの整形\n",
    "    output = df[['b_text','pose','a_text', 'category']]\n",
    "    \n",
    "    return output\n",
    "\n",
    "\"\"\"\n",
    "Window Size 4\n",
    "Maximum b_text length: 465 characters\n",
    "Maximum a_text length: 352 characters\n",
    "Window Size 3\n",
    "Maximum b_text length: 338 characters\n",
    "Maximum a_text length: 252 characters\n",
    "Window Size 2\n",
    "Maximum b_text length: 273 characters\n",
    "Maximum a_text length: 202 characters\n",
    "Window Size 1\n",
    "Maximum b_text length: 146 characters\n",
    "Maximum a_text length: 124 characters\n",
    "\"\"\"\n",
    "\n",
    "class ConjunctionDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=468):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        b_text = self.texts.iloc[idx]['b_text']\n",
    "        a_text = self.texts.iloc[idx]['a_text']\n",
    "        text = f\"{b_text} [SEP] {a_text}\"\n",
    "        label = self.labels.iloc[idx]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['@@@'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(output_list, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# トレーニングとテストデータに分割\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m train_texts, valid_texts, train_labels, valid_labels \u001b[38;5;241m=\u001b[39m train_test_split(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mb_text\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m@@@\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ma_text\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m, df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m'\u001b[39m], test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Tokenizerの準備\u001b[39;00m\n\u001b[0;32m     15\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m BertJapaneseTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcl-tohoku/bert-base-japanese\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\tarantula\\env\\Lib\\site-packages\\pandas\\core\\frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32mc:\\tarantula\\env\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\tarantula\\env\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['@@@'] not in index\""
     ]
    }
   ],
   "source": [
    "import glob\n",
    "from transformers import BertTokenizer, BertJapaneseTokenizer\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "file_paths = glob.glob('sentence_file/*')\n",
    "window_size = 1 # TODO: window_size = 1, 2, 3, 4 試す\n",
    "output_list = [make_dataset_before_tokenizer(file, window_size) for file in file_paths]\n",
    "df = pd.concat(output_list, ignore_index=True)\n",
    "# トレーニングとテストデータに分割\n",
    "train_texts, valid_texts, train_labels, valid_labels = train_test_split(df[['b_text','a_text']], df['category'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenizerの準備\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained('cl-tohoku/bert-base-japanese')\n",
    "\n",
    "# データローダーの準備\n",
    "train_dataset = ConjunctionDataset(train_texts, train_labels, tokenizer)\n",
    "valid_dataset = ConjunctionDataset(valid_texts, valid_labels, tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\tarantula\\env\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Train loss 1.789623327922543 accuracy 0.21981028821597956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [01:15<11:16, 75.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss 1.7897713004156601 accuracy 0.2137126185266229\n",
      "Epoch 2/10\n",
      "Train loss 1.7896208109730534 accuracy 0.21981028821597956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [02:27<09:46, 73.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss 1.7897713004156601 accuracy 0.2137126185266229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [02:28<09:52, 74.12s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 69\u001b[0m\n\u001b[0;32m     66\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(\u001b[38;5;28mrange\u001b[39m(epochs)): \u001b[38;5;66;03m# TODO: tqdmをいい感じの場所に置ければ置き直す\u001b[39;00m\n\u001b[1;32m---> 69\u001b[0m     train_acc, train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m accuracy \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[15], line 36\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, dataloader, loss_fn, optimizer, device, scheduler)\u001b[0m\n\u001b[0;32m     33\u001b[0m correct_predictions \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(preds \u001b[38;5;241m==\u001b[39m labels)\n\u001b[0;32m     34\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m---> 36\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     38\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\tarantula\\env\\Lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\tarantula\\env\\Lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import numpy as np\n",
    "from lstm_model import LSTMModel\n",
    "from lstm_model import BiLSTMModel\n",
    "import tqdm\n",
    "\n",
    "# モデルの準備\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BiLSTMModel(input_size=1, hidden_size=256, num_layers=2, output_size=6).to(device)  # 入力サイズはBERTの出力サイズ(768)、隠れ層サイズ、層数は例として指定\n",
    "\n",
    "# オプティマイザーとスケジューラーの設定\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
    "scheduler = StepLR(optimizer, step_size=2, gamma=0.1)\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "# 訓練用エポック関数\n",
    "def train_epoch(model, dataloader, loss_fn, optimizer, device, scheduler):\n",
    "    model = model.train()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for data in dataloader:\n",
    "        input_ids = data['input_ids'].to(device)\n",
    "        input_ids = input_ids.float()\n",
    "        input_ids = input_ids.unsqueeze(2)\n",
    "        labels = data['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids)\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        correct_predictions += torch.sum(preds == labels)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    return correct_predictions.double() / len(dataloader.dataset), np.mean(losses)\n",
    "\n",
    "# 検証用エポック関数\n",
    "def eval_model(model, dataloader, loss_fn, device):\n",
    "    model = model.eval()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            input_ids = data['input_ids'].to(device)\n",
    "            input_ids = input_ids.float()\n",
    "            input_ids = input_ids.unsqueeze(2) # バッチサイズ(16)*ウィンドウサイズ(465, 時系列のイメージ)*特徴量(1)\n",
    "            labels = data['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids)\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            correct_predictions += torch.sum(preds == labels)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    return correct_predictions.double() / len(dataloader.dataset), np.mean(losses)\n",
    "\n",
    "# トレーニングループ\n",
    "epochs = 10\n",
    "\n",
    "for epoch in tqdm.tqdm(range(epochs)): # TODO: tqdmをいい感じの場所に置ければ置き直す\n",
    "    train_acc, train_loss = train_epoch(model, train_dataloader, loss_fn, optimizer, device, scheduler)\n",
    "    print(f'Epoch {epoch + 1}/{epochs}')\n",
    "    print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "\n",
    "    val_acc, val_loss = eval_model(model, valid_dataloader, loss_fn, device)\n",
    "    print(f'Validation loss {val_loss} accuracy {val_acc}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport pandas as pd\\nfrom janome.tokenizer import Tokenizer\\nfrom transformers import BertJapaneseTokenizer\\nimport torch\\nfrom torch.utils.data import Dataset, DataLoader\\nfrom sklearn.model_selection import train_test_split\\n\\ndef load_and_preprocess_data(file_list, category_mapping, n_sentences):\\n    dfs = []\\n    for filename in file_list:\\n        df = pd.read_csv(f'sentence_file/{filename}')\\n        # データの整形\\n        df['text'] = df[['b4', 'b3', 'b2', 'b1', 'a1', 'a2', 'a3', 'a4'][:n_sentences*2]].apply(\\n            lambda x: ' '.join(x.dropna().astype(str)), axis=1)\\n        df = df[['text', 'category']]\\n        dfs.append(df)\\n    \\n    # 全データフレームを結合\\n    combined_df = pd.concat(dfs, ignore_index=True)\\n    \\n    # カテゴリを数値に変換\\n    combined_df['category'] = combined_df['category'].map(category_mapping) # MEMO: これいらない気がする\\n    \\n    return combined_df\\n\\nimport os\\n# dataフォルダ内の全ファイルに対して適用\\ndata_folder = 'sentence_file'\\nfile_list = []\\nfor filename in os.listdir(data_folder):\\n    if filename.endswith('.csv'):\\n        file_list.append(filename)\\n\\n# 前後何文を使用するか指定\\nn_sentences = 4\\n\\n# データの読み込みと前処理\\ndf = load_and_preprocess_data(file_list, category_mapping, n_sentences)\\n\\n# トレーニングとテストデータに分割\\ntrain_texts, test_texts, train_labels, test_labels = train_test_split(df['text'], df['category'], test_size=0.2, random_state=42)\\n\\n# Tokenizerの準備\\ntokenizer = BertJapaneseTokenizer.from_pretrained('cl-tohoku/bert-base-japanese')\\n\\nclass ConjunctionDataset(Dataset):\\n    def __init__(self, texts, labels, tokenizer, max_len=128):\\n        self.texts = texts\\n        self.labels = labels\\n        self.tokenizer = tokenizer\\n        self.max_len = max_len\\n\\n    def __len__(self):\\n        return len(self.texts)\\n\\n    def __getitem__(self, idx):\\n        text = self.texts.iloc[idx]\\n        label = self.labels.iloc[idx]\\n\\n        encoding = self.tokenizer.encode_plus(\\n            text,\\n            add_special_tokens=True,\\n            max_length=self.max_len,\\n            return_token_type_ids=False,\\n            padding='max_length',\\n            truncation=True,\\n            return_attention_mask=True,\\n            return_tensors='pt',\\n        )\\n\\n        return {\\n            'text': text,\\n            'input_ids': encoding['input_ids'].flatten(),\\n            'attention_mask': encoding['attention_mask'].flatten(),\\n            'labels': torch.tensor(label, dtype=torch.long)\\n        }\\n\\n# データローダーの準備\\ntrain_dataset = ConjunctionDataset(train_texts, train_labels, tokenizer)\\ntest_dataset = ConjunctionDataset(test_texts, test_labels, tokenizer)\\n\\ntrain_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\\ntest_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import pandas as pd\n",
    "from janome.tokenizer import Tokenizer\n",
    "from transformers import BertJapaneseTokenizer\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_and_preprocess_data(file_list, category_mapping, n_sentences):\n",
    "    dfs = []\n",
    "    for filename in file_list:\n",
    "        df = pd.read_csv(f'sentence_file/{filename}')\n",
    "        # データの整形\n",
    "        df['text'] = df[['b4', 'b3', 'b2', 'b1', 'a1', 'a2', 'a3', 'a4'][:n_sentences*2]].apply(\n",
    "            lambda x: ' '.join(x.dropna().astype(str)), axis=1)\n",
    "        df = df[['text', 'category']]\n",
    "        dfs.append(df)\n",
    "    \n",
    "    # 全データフレームを結合\n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    # カテゴリを数値に変換\n",
    "    combined_df['category'] = combined_df['category'].map(category_mapping) # MEMO: これいらない気がする\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "import os\n",
    "# dataフォルダ内の全ファイルに対して適用\n",
    "data_folder = 'sentence_file'\n",
    "file_list = []\n",
    "for filename in os.listdir(data_folder):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_list.append(filename)\n",
    "\n",
    "# 前後何文を使用するか指定\n",
    "n_sentences = 4\n",
    "\n",
    "# データの読み込みと前処理\n",
    "df = load_and_preprocess_data(file_list, category_mapping, n_sentences)\n",
    "\n",
    "# トレーニングとテストデータに分割\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(df['text'], df['category'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenizerの準備\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained('cl-tohoku/bert-base-japanese')\n",
    "\n",
    "class ConjunctionDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts.iloc[idx]\n",
    "        label = self.labels.iloc[idx]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# データローダーの準備\n",
    "train_dataset = ConjunctionDataset(train_texts, train_labels, tokenizer)\n",
    "test_dataset = ConjunctionDataset(test_texts, test_labels, tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport torch.nn as nn\\nfrom transformers import BertModel\\n\\nclass ConjunctionClassifier(nn.Module):\\n    def __init__(self, n_classes):\\n        super(ConjunctionClassifier, self).__init__()\\n        self.bert = BertModel.from_pretrained('cl-tohoku/bert-base-japanese')\\n        self.lstm = nn.LSTM(768, 128, batch_first=True, bidirectional=True)\\n        self.fc = nn.Linear(128*2, n_classes)\\n\\n    def forward(self, input_ids, attention_mask):\\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\\n        sequence_output, _ = self.lstm(outputs.last_hidden_state)\\n        avg_pool = torch.mean(sequence_output, 1)\\n        logits = self.fc(avg_pool)\\n        return logits\\n\\n# モデルの初期化\\nmodel = ConjunctionClassifier(n_classes=6)\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "\n",
    "class ConjunctionClassifier(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(ConjunctionClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('cl-tohoku/bert-base-japanese')\n",
    "        self.lstm = nn.LSTM(768, 128, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(128*2, n_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output, _ = self.lstm(outputs.last_hidden_state)\n",
    "        avg_pool = torch.mean(sequence_output, 1)\n",
    "        logits = self.fc(avg_pool)\n",
    "        return logits\n",
    "\n",
    "# モデルの初期化\n",
    "model = ConjunctionClassifier(n_classes=6)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom transformers import AdamW\\nfrom torch.optim.lr_scheduler import StepLR\\nimport numpy as np\\n\\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\\nmodel = model.to(device)\\n\\noptimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\\nscheduler = StepLR(optimizer, step_size=2, gamma=0.1)\\nloss_fn = nn.CrossEntropyLoss().to(device)\\n\\ndef train_epoch(model, dataloader, loss_fn, optimizer, device, scheduler):\\n    model = model.train()\\n    losses = []\\n    correct_predictions = 0\\n\\n    for data in dataloader:\\n        input_ids = data[\\'input_ids\\'].to(device)\\n        attention_mask = data[\\'attention_mask\\'].to(device)\\n        labels = data[\\'labels\\'].to(device)\\n\\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\\n        _, preds = torch.max(outputs, dim=1)\\n        loss = loss_fn(outputs, labels)\\n\\n        correct_predictions += torch.sum(preds == labels)\\n        losses.append(loss.item())\\n\\n        loss.backward()\\n        optimizer.step()\\n        scheduler.step()\\n        optimizer.zero_grad()\\n\\n    return correct_predictions.double() / len(dataloader.dataset), np.mean(losses)\\n\\ndef eval_model(model, dataloader, loss_fn, device):\\n    model = model.eval()\\n    losses = []\\n    correct_predictions = 0\\n\\n    with torch.no_grad():\\n        for data in dataloader:\\n            input_ids = data[\\'input_ids\\'].to(device)\\n            attention_mask = data[\\'attention_mask\\'].to(device)\\n            labels = data[\\'labels\\'].to(device)\\n\\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\\n            _, preds = torch.max(outputs, dim=1)\\n            loss = loss_fn(outputs, labels)\\n\\n            correct_predictions += torch.sum(preds == labels)\\n            losses.append(loss.item())\\n\\n    return correct_predictions.double() / len(dataloader.dataset), np.mean(losses)\\n\\n# トレーニングループ\\nepochs = 10\\n\\nfor epoch in range(epochs):\\n    train_acc, train_loss = train_epoch(model, train_dataloader, loss_fn, optimizer, device, scheduler)\\n    print(f\\'Epoch {epoch + 1}/{epochs}\\')\\n    print(f\\'Train loss {train_loss} accuracy {train_acc}\\')\\n\\n    val_acc, val_loss = eval_model(model, test_dataloader, loss_fn, device)\\n    print(f\\'Validation loss {val_loss} accuracy {val_acc}\\')\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from transformers import AdamW\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
    "scheduler = StepLR(optimizer, step_size=2, gamma=0.1)\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "def train_epoch(model, dataloader, loss_fn, optimizer, device, scheduler):\n",
    "    model = model.train()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for data in dataloader:\n",
    "        input_ids = data['input_ids'].to(device)\n",
    "        attention_mask = data['attention_mask'].to(device)\n",
    "        labels = data['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        correct_predictions += torch.sum(preds == labels)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    return correct_predictions.double() / len(dataloader.dataset), np.mean(losses)\n",
    "\n",
    "def eval_model(model, dataloader, loss_fn, device):\n",
    "    model = model.eval()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            input_ids = data['input_ids'].to(device)\n",
    "            attention_mask = data['attention_mask'].to(device)\n",
    "            labels = data['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            correct_predictions += torch.sum(preds == labels)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    return correct_predictions.double() / len(dataloader.dataset), np.mean(losses)\n",
    "\n",
    "# トレーニングループ\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_acc, train_loss = train_epoch(model, train_dataloader, loss_fn, optimizer, device, scheduler)\n",
    "    print(f'Epoch {epoch + 1}/{epochs}')\n",
    "    print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "\n",
    "    val_acc, val_loss = eval_model(model, test_dataloader, loss_fn, device)\n",
    "    print(f'Validation loss {val_loss} accuracy {val_acc}')\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
