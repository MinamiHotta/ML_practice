{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# beautifulsoup\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 時論公論(トップの10記事のみ)\n",
    "url = 'https://www.nhk.jp/p/ts/4V23PRP3YR/list/?pastOffset=10'\n",
    "# セットアップ\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service)\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【流れ】\n",
    "記事をクリック→もし「続きを読む」があったらクリック→タイトルを取得→記事本文を取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    }
   ],
   "source": [
    "title_list = []\n",
    "\n",
    "res = requests.get(url)\n",
    "\n",
    "wait = WebDriverWait(driver, 10)\n",
    "\n",
    "for num in range(1,11):\n",
    "    \n",
    "    article_x_path = f'//*[@id=\"past-pg\"]/ul/li[{num}]/div/div[3]/div[1]/a/div[1]/h3'\n",
    "    wait.until(EC.presence_of_element_located((By.XPATH, article_x_path)))\n",
    "    select_article = driver.find_element(By.XPATH, article_x_path)\n",
    "    select_article.click()\n",
    "    \n",
    "    more_read_x_path = '//*[@id=\"article-0\"]/div[3]/div[2]/a'\n",
    "    \n",
    "    try:\n",
    "        wait.until(EC.presence_of_element_located((By.XPATH, more_read_x_path)))\n",
    "        more_read_button = driver.find_element(By.XPATH, more_read_x_path)\n",
    "    \n",
    "    except: \n",
    "        driver.back()\n",
    "        sleep(1)\n",
    "        continue\n",
    "    \n",
    "    more_read_button.click()\n",
    "    sleep(2)\n",
    "    \n",
    "    # Seleniumでページのソースを取得(ボタンを押したあと＝レンダリング後のURLを取得したいのでSeleniumでurl取得)\n",
    "    page_source = driver.page_source\n",
    "    \n",
    "    # BeautifulSoupで解析\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "    # soup = BeautifulSoup(c, 'html.parser') # このページのタグを一斉に取得\n",
    "    \n",
    "    main = soup.find('article')\n",
    "    # タイトル取得\n",
    "    title = main.find('h1').text.strip()\n",
    "    title_list.append(title)\n",
    "    \n",
    "    paragraphs = main.find_all('p')\n",
    "    main_text = '\\n'.join([p.text.strip() for p in paragraphs])\n",
    "    \n",
    "    with open(f\"data/{title}.txt\", 'w', encoding='utf-8') as file:\n",
    "        file.write(main_text)\n",
    "    \n",
    "    driver.back()\n",
    "    sleep(1)\n",
    "\n",
    "# ブラウザを閉じる\n",
    "driver.quit()\n",
    "print(\"finish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(title_list).to_csv('data/時論公論_title.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
